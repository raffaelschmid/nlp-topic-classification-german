{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_parquet\n",
    "from data import file\n",
    "\n",
    "data_train = read_parquet(file.news_articles_cleaned_train)\n",
    "data_test = read_parquet(file.news_articles_cleaned_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "import fasttext.util\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, InputLayer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import Sequential\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from preprocessing.text import extract_vocabulary\n",
    "\n",
    "\n",
    "def get_embedder_fasttext(embedding_dim, model_name=\"cc.de.300.bin\"):\n",
    "    split = model_name.split(\".\")\n",
    "    model_lang = split[1]\n",
    "    model_dim = int(split[2])\n",
    "\n",
    "    try:\n",
    "        ft = fasttext.load_model(model_name)\n",
    "    except ValueError:\n",
    "        fasttext.util.download_model(model_lang, if_exists='ignore')\n",
    "        ft = fasttext.load_model(model_name)\n",
    "\n",
    "    if embedding_dim < model_dim:\n",
    "        fasttext.util.reduce_model(ft, embedding_dim)\n",
    "\n",
    "    def fasttext_embedder(word):\n",
    "        return ft.get_word_vector(word)\n",
    "\n",
    "    return fasttext_embedder\n",
    "\n",
    "\n",
    "def calculate_embedding_matrix(vocabulary, embedding_dim=300, verbose=False):\n",
    "    \"\"\"Creates the embedding matrix\n",
    "    \"\"\"\n",
    "    voc_size = len(vocabulary)\n",
    "    words_not_found = set()\n",
    "    embedding_matrix = np.zeros((voc_size, embedding_dim))\n",
    "\n",
    "    embedder = get_embedder_fasttext(embedding_dim)\n",
    "    \n",
    "    \n",
    "    for idx, word in enumerate(vocabulary):\n",
    "        embedding_vector = embedder(word)\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0 and not np.all(embedding_vector == 0):\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        else:\n",
    "            words_not_found.add(word)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Embedding type: fasttext\")\n",
    "        print(\"Number of null word embeddings:\", np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "        nr_words_not_found = len(words_not_found)\n",
    "        print(\"Words not found in total:\", len(words_not_found))\n",
    "        if nr_words_not_found > 0:\n",
    "            import random\n",
    "\n",
    "            nr_sample = min(20, len(words_not_found))\n",
    "            print(\"Words without embedding (\", nr_sample, \"/\", nr_words_not_found, \"): \",\n",
    "                  random.sample(words_not_found, nr_sample), sep='')\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def compile_model(model, loss_function=\"categorical_crossentropy\", learning_rate=0.01, model_metric = [\"accuracy\"]):\n",
    "    adam = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=loss_function, optimizer=adam, metrics=model_metric)\n",
    "    \n",
    "    \n",
    "def build_model_cnn_simple(X_train, y_train, conv_num_filters=128, conv_kernel_size=7):\n",
    "\n",
    "    vocabulary, embedding_length = extract_vocabulary(X_train, verbose=True)\n",
    "    print(f\"embedding length: {embedding_length}\")\n",
    "    \n",
    "    embedding_matrix = calculate_embedding_matrix(vocabulary)\n",
    "    embedding_input_dim, embedding_output_dim = embedding_matrix.shape[0], embedding_matrix.shape[1]\n",
    "    \n",
    "    output_classes = len(y_train.unique())\n",
    "\n",
    "    vectorize_layer = TextVectorization(\n",
    "        output_mode='int',\n",
    "        output_sequence_length=None,\n",
    "        vocabulary=list(vocabulary),\n",
    "        name=\"text_vectorization\"\n",
    "    )\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        embedding_input_dim,\n",
    "        embedding_output_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=embedding_length,\n",
    "        trainable=False,\n",
    "        mask_zero=True,\n",
    "        name=\"embedding\"\n",
    "    )\n",
    "\n",
    "\n",
    "    model = Sequential(name=\"cnn\")\n",
    "    model.add(InputLayer(input_shape=(1,), dtype=tf.string, name=\"text_input\"))\n",
    "    model.add(vectorize_layer)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(conv_num_filters, conv_kernel_size, activation=\"relu\", strides=1, padding=\"valid\", name=\"conv_1\"))\n",
    "    model.add(GlobalMaxPooling1D(name=\"global_max_pool_1\"))\n",
    "    model.add(Dense(output_classes, activation=tf.nn.softmax, name=\"prediction\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.text_tokenized_stemmed\n",
    "y_train = data_train.label\n",
    "\n",
    "X_test = data_test.text_tokenized_stemmed\n",
    "y_test = data_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median sequence length:       : 173\n",
      "Percentil                     : 0.98)\n",
      "Cutoff sequence length        : 589\n",
      "Max sequence length           : 1699\n",
      "Embedding length              : 589\n",
      "Vocabulary length             : 162207\n",
      "embedding length: 589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "2021-11-13 07:40:18.738324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 07:40:18.816033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 07:40:18.816723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 07:40:18.818611: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-13 07:40:18.820460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 07:40:18.821090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 07:40:18.821659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 07:40:20.932930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 07:40:20.933630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 07:40:20.934220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 07:40:20.937715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13839 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 300)         48662100  \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv1D)              (None, None, 128)         268928    \n",
      "_________________________________________________________________\n",
      "global_max_pool_1 (GlobalMax (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 48,932,189\n",
      "Trainable params: 270,089\n",
      "Non-trainable params: 48,662,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model_cnn_simple(X_train, y_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "y_train = data_train.label\n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(y_train)\n",
    "\n",
    "x_train = data_train['text_stem']\n",
    "y_train_bin = label_binarizer.transform(y_train)\n",
    "\n",
    "x_test = data_test['text_stem']\n",
    "y_test_bin = label_binarizer.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None,), (None, 9)), types: (tf.string, tf.int64)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_input = tf.data.Dataset.from_tensor_slices((x_train, y_train_bin)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_input = tf.data.Dataset.from_tensor_slices((x_test, y_test_bin)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "train_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "#%tensorboard --logdir logs/fit\n",
    "\n",
    "#callbacks = [TensorBoard(\"logs/fit\", histogram_freq=1)]\n",
    "callbacks = []\n",
    "history = model.fit(train_input, validation_data=test_input, callbacks=callbacks, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reporting.training import plot_history\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = label_binarizer.inverse_transform(model.predict(x_test[0:100]))\n",
    "\n",
    "\n",
    "from reporting.evaluation import plot_confusion_matrix\n",
    "plot_confusion_matrix(y_test[0:100], y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
