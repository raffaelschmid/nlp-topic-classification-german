{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_parquet\n",
    "from data import file\n",
    "\n",
    "data_train = read_parquet(file.news_articles_cleaned_train)\n",
    "data_test = read_parquet(file.news_articles_cleaned_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'extract_vocabulary' from 'preprocessing.text' (/home/jupyter/code/nlp-topic-classification-german/preprocessing/text.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14096/3589204810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'extract_vocabulary' from 'preprocessing.text' (/home/jupyter/code/nlp-topic-classification-german/preprocessing/text.py)"
     ]
    }
   ],
   "source": [
    "from keras.layers import TextVectorization\n",
    "import fasttext.util\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, InputLayer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import Sequential\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from preprocessing.text import extract_vocabulary\n",
    "\n",
    "\n",
    "def get_embedder_fasttext(embedding_dim, model_name=\"cc.de.300.bin\"):\n",
    "    split = model_name.split(\".\")\n",
    "    model_lang = split[1]\n",
    "    model_dim = int(split[2])\n",
    "\n",
    "    try:\n",
    "        ft = fasttext.load_model(model_name)\n",
    "    except ValueError:\n",
    "        fasttext.util.download_model(model_lang, if_exists='ignore')\n",
    "        ft = fasttext.load_model(model_name)\n",
    "\n",
    "    if embedding_dim < model_dim:\n",
    "        fasttext.util.reduce_model(ft, embedding_dim)\n",
    "\n",
    "    def fasttext_embedder(word):\n",
    "        return ft.get_word_vector(word)\n",
    "\n",
    "    return fasttext_embedder\n",
    "\n",
    "\n",
    "def calculate_embedding_matrix(vocabulary, embedding_dim=300, verbose=False):\n",
    "    \"\"\"Creates the embedding matrix\n",
    "    \"\"\"\n",
    "    voc_size = len(vocabulary)\n",
    "    words_not_found = set()\n",
    "    embedding_matrix = np.zeros((voc_size, embedding_dim))\n",
    "\n",
    "    embedder = get_embedder_fasttext(embedding_dim)\n",
    "    \n",
    "    \n",
    "    for idx, word in enumerate(vocabulary):\n",
    "        embedding_vector = embedder(word)\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0 and not np.all(embedding_vector == 0):\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        else:\n",
    "            words_not_found.add(word)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Embedding type: fasttext\")\n",
    "        print(\"Number of null word embeddings:\", np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "        nr_words_not_found = len(words_not_found)\n",
    "        print(\"Words not found in total:\", len(words_not_found))\n",
    "        if nr_words_not_found > 0:\n",
    "            import random\n",
    "\n",
    "            nr_sample = min(20, len(words_not_found))\n",
    "            print(\"Words without embedding (\", nr_sample, \"/\", nr_words_not_found, \"): \",\n",
    "                  random.sample(words_not_found, nr_sample), sep='')\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def compile_model(model, loss_function=\"categorical_crossentropy\", learning_rate=0.01, model_metric = [\"accuracy\"]):\n",
    "    adam = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=loss_function, optimizer=adam, metrics=model_metric)\n",
    "    \n",
    "    \n",
    "def build_model_cnn_simple(X_train, y_train, conv_num_filters=128, conv_kernel_size=7):\n",
    "\n",
    "    vocabulary, embedding_length = extract_vocabulary(X_train, verbose=True)\n",
    "    print(f\"embedding length: {embedding_length}\")\n",
    "    \n",
    "    embedding_matrix = calculate_embedding_matrix(vocabulary)\n",
    "    embedding_input_dim, embedding_output_dim = embedding_matrix.shape[0], embedding_matrix.shape[1]\n",
    "    \n",
    "    output_classes = len(y_train.unique())\n",
    "\n",
    "    vectorize_layer = TextVectorization(\n",
    "        output_mode='int',\n",
    "        output_sequence_length=None,\n",
    "        vocabulary=list(vocabulary),\n",
    "        name=\"text_vectorization\"\n",
    "    )\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        embedding_input_dim,\n",
    "        embedding_output_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=embedding_length,\n",
    "        trainable=False,\n",
    "        mask_zero=True,\n",
    "        name=\"embedding\"\n",
    "    )\n",
    "\n",
    "\n",
    "    model = Sequential(name=\"cnn\")\n",
    "    model.add(InputLayer(input_shape=(1,), dtype=tf.string, name=\"text_input\"))\n",
    "    model.add(vectorize_layer)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(conv_num_filters, conv_kernel_size, activation=\"relu\", strides=1, padding=\"valid\", name=\"conv_1\"))\n",
    "    model.add(GlobalMaxPooling1D(name=\"global_max_pool_1\"))\n",
    "    model.add(Dense(output_classes, activation=tf.nn.softmax, name=\"prediction\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'preprocessing.text' has no attribute 'extract_focabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14096/2518072950.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfoo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_focabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'preprocessing.text' has no attribute 'extract_focabulary'"
     ]
    }
   ],
   "source": [
    "import preprocessing.text as foo\n",
    "foo.extract_focabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.text_tokenized_stemmed\n",
    "y_train = data_train.label\n",
    "\n",
    "X_test = data_test.text_tokenized_stemmed\n",
    "y_test = data_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = build_model_cnn_simple(X_train, y_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "y_train = data_train.label\n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(y_train)\n",
    "\n",
    "x_train = data_train['text_stem']\n",
    "y_train_bin = label_binarizer.transform(y_train)\n",
    "\n",
    "x_test = data_test['text_stem']\n",
    "y_test_bin = label_binarizer.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_input = tf.data.Dataset.from_tensor_slices((x_train, y_train_bin)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_input = tf.data.Dataset.from_tensor_slices((x_test, y_test_bin)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "train_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/fit\n",
    "\n",
    "#callbacks = [TensorBoard(\"logs/fit\", histogram_freq=1)]\n",
    "callbacks = []\n",
    "history = model.fit(train_input, validation_data=test_input, callbacks=callbacks, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reporting.training import plot_history\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = label_binarizer.inverse_transform(model.predict(x_test[0:100]))\n",
    "\n",
    "\n",
    "from reporting.evaluation import plot_confusion_matrix\n",
    "plot_confusion_matrix(y_test[0:100], y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
