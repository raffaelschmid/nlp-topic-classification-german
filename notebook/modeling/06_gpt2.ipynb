{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cdc79c83-60dd-4126-aded-dd8823b1c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "from pandas import read_parquet, DataFrame, Series, concat\n",
    "from data import file\n",
    "from tqdm import tqdm\n",
    "from preprocessing.categorical import binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1a75c19-b8a0-414d-8079-fc0f5da186ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply progress bar on pandas operations\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afb29060-32f8-42dd-ac68-64a1219a0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee96e5d5-bec8-487f-a8c9-0e8754b3d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = read_parquet(file.news_articles_cleaned_train)\n",
    "data_test = read_parquet(file.news_articles_cleaned_test)\n",
    "data_val = read_parquet(file.news_articles_cleaned_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c2793fd-2bdc-4dc1-b3b7-3c8683b7837e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (7191, 9) categories: 9\n",
      "test size : (2054, 9) categories: 9\n",
      "val size  : (1028, 9) categories: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"train size:\", data_train.shape, \"categories:\", len(data_train.label.unique()))\n",
    "print(\"test size :\", data_test.shape, \"categories:\", len(data_test.label.unique()))\n",
    "print(\"val size  :\", data_val.shape, \"categories:\", len(data_val.label.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b1a036f-56bc-4c69-bf66-0f237b876584",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 192\n",
    "\n",
    "def tokenize(review):\n",
    "\n",
    "  encoded = tokenizer.encode_plus(\n",
    "      text=review,\n",
    "      add_special_tokens=True,     # Add `[CLS]` and `[SEP]`\n",
    "      max_length=MAXLEN,           # Max length to truncate/pad\n",
    "      padding='max_length',        # Pad sentence to max length\n",
    "      return_attention_mask=False, # attention mask not needed for our task\n",
    "      return_token_type_ids=False,\n",
    "      truncation=True)\n",
    "  return encoded['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4ee786d-420e-48ad-8ee2-6e858123f2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7191/7191 [00:52<00:00, 135.87it/s]\n"
     ]
    }
   ],
   "source": [
    "data_hf_tokenized_train = concat([data_train, data_train.text_original.progress_map(tokenize).rename('hf_tokenized')], axis=1)\n",
    "data_hf_tokenized_train.to_parquet(path=file.news_articles_hf_tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbf5b3f6-0106-477b-8027-257fc1375ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2054/2054 [00:15<00:00, 131.58it/s]\n"
     ]
    }
   ],
   "source": [
    "data_hf_tokenized_test = concat([data_test, data_test.text_original.progress_map(tokenize).rename('hf_tokenized')], axis=1)\n",
    "data_hf_tokenized_test.to_parquet(path=file.news_articles_hf_tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "593dbaaf-a08c-4594-b4ca-9405b9e21dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1028/1028 [00:07<00:00, 131.21it/s]\n"
     ]
    }
   ],
   "source": [
    "data_hf_tokenized_val = concat([data_val, data_val.text_original.progress_map(tokenize).rename('hf_tokenized')], axis=1)\n",
    "data_hf_tokenized_val.to_parquet(path=file.news_articles_hf_tokenized_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "946ba12e-009d-4057-950e-5335b7e5609f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (7191, 10) categories: 9\n",
      "test size : (2054, 10) categories: 9\n",
      "val size  : (1028, 10) categories: 9\n"
     ]
    }
   ],
   "source": [
    "hf_data_train = read_parquet(file.news_articles_hf_tokenized_train)\n",
    "hf_data_test = read_parquet(file.news_articles_hf_tokenized_test)\n",
    "hf_data_val = read_parquet(file.news_articles_hf_tokenized_val)\n",
    "\n",
    "print(\"train size:\", hf_data_train.shape, \"categories:\", len(hf_data_train.label.unique()))\n",
    "print(\"test size :\", hf_data_test.shape, \"categories:\", len(hf_data_test.label.unique()))\n",
    "print(\"val size  :\", hf_data_val.shape, \"categories:\", len(hf_data_val.label.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4da3a779-32db-4924-b2f8-40f30bdf1a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train label size: (7191, 9) categories: 9\n",
      "test label size : (2054, 9) categories: 9\n"
     ]
    }
   ],
   "source": [
    "label_binarizer=binarizer(hf_data_train.label)\n",
    "label_bin_train = label_binarizer.transform(hf_data_train.label)\n",
    "label_bin_test = label_binarizer.transform(hf_data_test.label)\n",
    "\n",
    "print(\"train label size:\", label_bin_train.shape, \"categories:\", len(hf_data_train.label.unique()))\n",
    "print(\"test label size :\", label_bin_test.shape, \"categories:\", len(hf_data_test.label.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2bb9bbc8-f870-4519-a3cd-13d0533f0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices((hf_data_train.hf_tokenized.map(lambda x:x.tolist()).tolist(), label_bin_train))\n",
    "                .batch(BATCH_SIZE)\n",
    "                .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices((hf_data_test.hf_tokenized.map(lambda x:x.tolist()).tolist(), label_bin_test))\n",
    "                .batch(BATCH_SIZE)\n",
    "                .prefetch(tf.data.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "085d0686-e7b3-482b-8c0e-861e9c010895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(output_classes, max_len=MAXLEN):\n",
    "    \"\"\" add binary classification to pretrained model\n",
    "    \"\"\"\n",
    "\n",
    "    input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\"\n",
    "    )\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-german-cased\")\n",
    "    encoder_outputs = bert_model(input_word_ids)\n",
    "\n",
    "    pooler_output = encoder_outputs[1]\n",
    "    cls_embedding = pooler_output\n",
    "\n",
    "    stack = tf.keras.layers.Dense(output_classes)(cls_embedding)\n",
    "    output = tf.keras.layers.Activation('softmax')(stack)\n",
    "\n",
    "    ##########################\n",
    "    ## YOUR CODE HERE END ##\n",
    "    ##########################\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=input_word_ids, outputs=output)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6d07e130-cf5d-4d31-a77b-be6b9e81e54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-german-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-german-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
      "_________________________________________________________________\n",
      "tf_bert_model_2 (TFBertModel TFBaseModelOutputWithPool 109081344 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 6921      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 109,088,265\n",
      "Trainable params: 109,088,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(len(data_train.label.unique()), max_len=MAXLEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3ba58758-439f-4251-bc3d-0d165b287d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = loss=\"binary_crossentropy\"\n",
    "\n",
    "model.compile(optimizer, loss=loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3f89771e-dcf6-44a1-b4cf-76a67613cecd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22732/1206934492.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_data_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.floor((len(hf_data_train) / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d8e92d-4278-4899-98f4-c14e91e17f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=int(np.floor((len(hf_data_train) / BATCH_SIZE))),\n",
    "    validation_data=test_dataset,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "               tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", verbose=1, patience=1, restore_best_weights=True),\n",
    "               tf.keras.callbacks.TensorBoard(f'logs/{datetime.now()}')\n",
    "               ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
