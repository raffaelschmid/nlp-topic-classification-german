{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from fhnw.nlp.utils.storage import load_dataframe\n",
    "from pandas import concat\n",
    "\n",
    "from data import file\n",
    "from preprocessing.text import tokenize\n",
    "from preprocessing.token import ignore_stopwords\n",
    "from preprocessing.token import stem, lemma"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_train = load_dataframe(file.news_articles_raw_train)\n",
    "data_test = load_dataframe(file.news_articles_raw_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "tokenized_train = data_train.text_original.map(tokenize).rename('text_tokenized')\n",
    "tokenized_test = data_test.text_original.map(tokenize).rename('text_tokenized')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "keywords_train = tokenized_train.map(ignore_stopwords).rename('text_tokenized_keywords')\n",
    "keywords_test = tokenized_test.map(ignore_stopwords).rename('text_tokenized_keywords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "lemmas_train = keywords_train.map(lemma).rename('text_tokenized_lemmas')\n",
    "lemmas_test = keywords_test.map(lemma).rename('text_tokenized_lemmas')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "stem_train = keywords_train.map(stem).rename('text_tokenized_stemmed')\n",
    "stem_test = keywords_test.map(stem).rename('text_tokenized_stemmed')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "result_train = concat([data_train, tokenized_train, keywords_train, lemmas_train, stem_train], axis=1)\n",
    "result_test = concat([data_test, tokenized_test, keywords_test, lemmas_test, stem_test], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "result_train.to_parquet(path=file.news_articles_cleaned_train)\n",
    "result_test.to_parquet(path=file.news_articles_cleaned_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for label in keywords_train[\"label\"].unique():\n",
    "#     tokens = keywords_train.loc[(data_train_tokenized[\"label\"] == label)].text_tokenized\n",
    "#     create_word_cloud(tokens)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}