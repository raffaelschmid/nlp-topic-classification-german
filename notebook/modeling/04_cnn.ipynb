{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_parquet\n",
    "from data import file\n",
    "\n",
    "data_train = read_parquet(file.news_articles_cleaned_train)\n",
    "data_test = read_parquet(file.news_articles_cleaned_test)\n",
    "data_val = read_parquet(file.news_articles_cleaned_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_original</th>\n",
       "      <th>label</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_tokenized_keywords</th>\n",
       "      <th>text_keywords</th>\n",
       "      <th>text_tokenized_lemmas</th>\n",
       "      <th>text_lemmas</th>\n",
       "      <th>text_tokenized_stemmed</th>\n",
       "      <th>text_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>Asylverfahren sollen verkürzt werden, dafür is...</td>\n",
       "      <td>Wirtschaft</td>\n",
       "      <td>[asylverfahren, sollen, verkürzt, werden, ,, d...</td>\n",
       "      <td>[asylverfahren, sollen, verkürzt, ,, dafür, ge...</td>\n",
       "      <td>asylverfahren sollen verkürzt , dafür gewerksc...</td>\n",
       "      <td>[asylverfahren, sollen, verkürzen, dafür, gewe...</td>\n",
       "      <td>asylverfahren sollen verkürzen dafür gewerksch...</td>\n",
       "      <td>[asylverfahr, soll, verkurzt, dafur, gewerksch...</td>\n",
       "      <td>asylverfahr soll verkurzt dafur gewerkschaft b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7445</th>\n",
       "      <td>Umsatz im abgelaufenen Quartal fiel um 8 Proze...</td>\n",
       "      <td>Web</td>\n",
       "      <td>[umsatz, im, abgelaufenen, quartal, fiel, um, ...</td>\n",
       "      <td>[umsatz, abgelaufenen, quartal, fiel, 8, proze...</td>\n",
       "      <td>umsatz abgelaufenen quartal fiel 8 prozent 12,...</td>\n",
       "      <td>[umsatz, abgelaufen, quartal, fallen, 8, proze...</td>\n",
       "      <td>umsatz abgelaufen quartal fallen 8 prozent 12,...</td>\n",
       "      <td>[umsatz, abgelauf, quartal, fiel, 8, prozent, ...</td>\n",
       "      <td>umsatz abgelauf quartal fiel 8 prozent 12,9 mr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8808</th>\n",
       "      <td>Wegen Nichterfüllung eines Lizenzkriteriums in...</td>\n",
       "      <td>Sport</td>\n",
       "      <td>[wegen, nichterfüllung, eines, lizenzkriterium...</td>\n",
       "      <td>[wegen, nichterfüllung, lizenzkriteriums, stad...</td>\n",
       "      <td>wegen nichterfüllung lizenzkriteriums stadionf...</td>\n",
       "      <td>[wegen, nichterfüllung, lizenzkriteriums, stad...</td>\n",
       "      <td>wegen nichterfüllung lizenzkriteriums stadionf...</td>\n",
       "      <td>[weg, nichterfull, lizenzkriterium, stadionfra...</td>\n",
       "      <td>weg nichterfull lizenzkriterium stadionfrag na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>Demokratische Präsidentschaftsanwärterin forde...</td>\n",
       "      <td>International</td>\n",
       "      <td>[demokratische, präsidentschaftsanwärterin, fo...</td>\n",
       "      <td>[demokratische, präsidentschaftsanwärterin, fo...</td>\n",
       "      <td>demokratische präsidentschaftsanwärterin forde...</td>\n",
       "      <td>[demokratische, präsidentschaftsanwärterin, fo...</td>\n",
       "      <td>demokratische präsidentschaftsanwärterin forde...</td>\n",
       "      <td>[demokrat, prasidentschaftsanwarterin, fordert...</td>\n",
       "      <td>demokrat prasidentschaftsanwarterin fordert ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>Republikaner: Transparenzregeln umgangen. Die ...</td>\n",
       "      <td>Web</td>\n",
       "      <td>[republikaner, :, transparenzregeln, umgangen,...</td>\n",
       "      <td>[republikaner, :, transparenzregeln, umgangen,...</td>\n",
       "      <td>republikaner : transparenzregeln umgangen . us...</td>\n",
       "      <td>[republikaner, transparenzregeln, umgangen, us...</td>\n",
       "      <td>republikaner transparenzregeln umgangen us-prä...</td>\n",
       "      <td>[republikan, transparenzregeln, umgang, us-pra...</td>\n",
       "      <td>republikan transparenzregeln umgang us-praside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5431</th>\n",
       "      <td>Premier Alexis Tsipras, der ein neuerliches Vo...</td>\n",
       "      <td>International</td>\n",
       "      <td>[premier, alexis, tsipras, ,, der, ein, neuerl...</td>\n",
       "      <td>[premier, alexis, tsipras, ,, neuerliches, vot...</td>\n",
       "      <td>premier alexis tsipras , neuerliches votum aus...</td>\n",
       "      <td>[premier, alexis, tsipras, neuerlich, votum, a...</td>\n",
       "      <td>premier alexis tsipras neuerlich votum ausschl...</td>\n",
       "      <td>[premi, alexis, tsipras, neu, votum, ausgeschl...</td>\n",
       "      <td>premi alexis tsipras neu votum ausgeschloss st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>Ein Kreditereignis bei BMW sollte ausbleiben. ...</td>\n",
       "      <td>Wirtschaft</td>\n",
       "      <td>[ein, kreditereignis, bei, bmw, sollte, ausble...</td>\n",
       "      <td>[kreditereignis, bmw, ausbleiben, ., bmw, bedi...</td>\n",
       "      <td>kreditereignis bmw ausbleiben . bmw bedient ma...</td>\n",
       "      <td>[kreditereignis, bmw, ausbleiben, bmw, bediene...</td>\n",
       "      <td>kreditereignis bmw ausbleiben bmw bedienen mar...</td>\n",
       "      <td>[kreditereignis, bmw, ausbleib, bmw, bedient, ...</td>\n",
       "      <td>kreditereignis bmw ausbleib bmw bedient markt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3642</th>\n",
       "      <td>Fledermäuse sind von vielen Krankheitserregern...</td>\n",
       "      <td>Wissenschaft</td>\n",
       "      <td>[fledermäuse, sind, von, vielen, krankheitserr...</td>\n",
       "      <td>[fledermäuse, vielen, krankheitserregern, ,, s...</td>\n",
       "      <td>fledermäuse vielen krankheitserregern , säuget...</td>\n",
       "      <td>[fledermäuse, viel, krankheitserregern, säuget...</td>\n",
       "      <td>fledermäuse viel krankheitserregern säugetiere...</td>\n",
       "      <td>[fledermaus, viel, krankheitserreg, saugeti, b...</td>\n",
       "      <td>fledermaus viel krankheitserreg saugeti befall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8420</th>\n",
       "      <td>Ein Fossil aus Spanien mit \"Star Wars\"-Bezug. ...</td>\n",
       "      <td>Wissenschaft</td>\n",
       "      <td>[ein, fossil, aus, spanien, mit, ``, star, war...</td>\n",
       "      <td>[fossil, spanien, ``, star, wars, '', -bezug, ...</td>\n",
       "      <td>fossil spanien `` star wars '' -bezug . madrid...</td>\n",
       "      <td>[fossil, spanien, ``, star, wars, -bezug, madr...</td>\n",
       "      <td>fossil spanien `` star wars -bezug madrid wien...</td>\n",
       "      <td>[fossil, spani, ``, star, war, '', -bezug, mad...</td>\n",
       "      <td>fossil spani `` star war '' -bezug madrid/wi k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6661</th>\n",
       "      <td>General Motors und Ford setzen mehr Fahrzeuge ...</td>\n",
       "      <td>Wirtschaft</td>\n",
       "      <td>[general, motors, und, ford, setzen, mehr, fah...</td>\n",
       "      <td>[general, motors, ford, setzen, mehr, fahrzeug...</td>\n",
       "      <td>general motors ford setzen mehr fahrzeuge ab –...</td>\n",
       "      <td>[general, motors, ford, setzen, mehr, fahrzeug...</td>\n",
       "      <td>general motors ford setzen mehr fahrzeuge ab u...</td>\n",
       "      <td>[general, motor, ford, setz, mehr, fahrzeug, a...</td>\n",
       "      <td>general motor ford setz mehr fahrzeug ab us-au...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7191 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_original          label  \\\n",
       "1637  Asylverfahren sollen verkürzt werden, dafür is...     Wirtschaft   \n",
       "7445  Umsatz im abgelaufenen Quartal fiel um 8 Proze...            Web   \n",
       "8808  Wegen Nichterfüllung eines Lizenzkriteriums in...          Sport   \n",
       "1982  Demokratische Präsidentschaftsanwärterin forde...  International   \n",
       "255   Republikaner: Transparenzregeln umgangen. Die ...            Web   \n",
       "...                                                 ...            ...   \n",
       "5431  Premier Alexis Tsipras, der ein neuerliches Vo...  International   \n",
       "2368  Ein Kreditereignis bei BMW sollte ausbleiben. ...     Wirtschaft   \n",
       "3642  Fledermäuse sind von vielen Krankheitserregern...   Wissenschaft   \n",
       "8420  Ein Fossil aus Spanien mit \"Star Wars\"-Bezug. ...   Wissenschaft   \n",
       "6661  General Motors und Ford setzen mehr Fahrzeuge ...     Wirtschaft   \n",
       "\n",
       "                                         text_tokenized  \\\n",
       "1637  [asylverfahren, sollen, verkürzt, werden, ,, d...   \n",
       "7445  [umsatz, im, abgelaufenen, quartal, fiel, um, ...   \n",
       "8808  [wegen, nichterfüllung, eines, lizenzkriterium...   \n",
       "1982  [demokratische, präsidentschaftsanwärterin, fo...   \n",
       "255   [republikaner, :, transparenzregeln, umgangen,...   \n",
       "...                                                 ...   \n",
       "5431  [premier, alexis, tsipras, ,, der, ein, neuerl...   \n",
       "2368  [ein, kreditereignis, bei, bmw, sollte, ausble...   \n",
       "3642  [fledermäuse, sind, von, vielen, krankheitserr...   \n",
       "8420  [ein, fossil, aus, spanien, mit, ``, star, war...   \n",
       "6661  [general, motors, und, ford, setzen, mehr, fah...   \n",
       "\n",
       "                                text_tokenized_keywords  \\\n",
       "1637  [asylverfahren, sollen, verkürzt, ,, dafür, ge...   \n",
       "7445  [umsatz, abgelaufenen, quartal, fiel, 8, proze...   \n",
       "8808  [wegen, nichterfüllung, lizenzkriteriums, stad...   \n",
       "1982  [demokratische, präsidentschaftsanwärterin, fo...   \n",
       "255   [republikaner, :, transparenzregeln, umgangen,...   \n",
       "...                                                 ...   \n",
       "5431  [premier, alexis, tsipras, ,, neuerliches, vot...   \n",
       "2368  [kreditereignis, bmw, ausbleiben, ., bmw, bedi...   \n",
       "3642  [fledermäuse, vielen, krankheitserregern, ,, s...   \n",
       "8420  [fossil, spanien, ``, star, wars, '', -bezug, ...   \n",
       "6661  [general, motors, ford, setzen, mehr, fahrzeug...   \n",
       "\n",
       "                                          text_keywords  \\\n",
       "1637  asylverfahren sollen verkürzt , dafür gewerksc...   \n",
       "7445  umsatz abgelaufenen quartal fiel 8 prozent 12,...   \n",
       "8808  wegen nichterfüllung lizenzkriteriums stadionf...   \n",
       "1982  demokratische präsidentschaftsanwärterin forde...   \n",
       "255   republikaner : transparenzregeln umgangen . us...   \n",
       "...                                                 ...   \n",
       "5431  premier alexis tsipras , neuerliches votum aus...   \n",
       "2368  kreditereignis bmw ausbleiben . bmw bedient ma...   \n",
       "3642  fledermäuse vielen krankheitserregern , säuget...   \n",
       "8420  fossil spanien `` star wars '' -bezug . madrid...   \n",
       "6661  general motors ford setzen mehr fahrzeuge ab –...   \n",
       "\n",
       "                                  text_tokenized_lemmas  \\\n",
       "1637  [asylverfahren, sollen, verkürzen, dafür, gewe...   \n",
       "7445  [umsatz, abgelaufen, quartal, fallen, 8, proze...   \n",
       "8808  [wegen, nichterfüllung, lizenzkriteriums, stad...   \n",
       "1982  [demokratische, präsidentschaftsanwärterin, fo...   \n",
       "255   [republikaner, transparenzregeln, umgangen, us...   \n",
       "...                                                 ...   \n",
       "5431  [premier, alexis, tsipras, neuerlich, votum, a...   \n",
       "2368  [kreditereignis, bmw, ausbleiben, bmw, bediene...   \n",
       "3642  [fledermäuse, viel, krankheitserregern, säuget...   \n",
       "8420  [fossil, spanien, ``, star, wars, -bezug, madr...   \n",
       "6661  [general, motors, ford, setzen, mehr, fahrzeug...   \n",
       "\n",
       "                                            text_lemmas  \\\n",
       "1637  asylverfahren sollen verkürzen dafür gewerksch...   \n",
       "7445  umsatz abgelaufen quartal fallen 8 prozent 12,...   \n",
       "8808  wegen nichterfüllung lizenzkriteriums stadionf...   \n",
       "1982  demokratische präsidentschaftsanwärterin forde...   \n",
       "255   republikaner transparenzregeln umgangen us-prä...   \n",
       "...                                                 ...   \n",
       "5431  premier alexis tsipras neuerlich votum ausschl...   \n",
       "2368  kreditereignis bmw ausbleiben bmw bedienen mar...   \n",
       "3642  fledermäuse viel krankheitserregern säugetiere...   \n",
       "8420  fossil spanien `` star wars -bezug madrid wien...   \n",
       "6661  general motors ford setzen mehr fahrzeuge ab u...   \n",
       "\n",
       "                                 text_tokenized_stemmed  \\\n",
       "1637  [asylverfahr, soll, verkurzt, dafur, gewerksch...   \n",
       "7445  [umsatz, abgelauf, quartal, fiel, 8, prozent, ...   \n",
       "8808  [weg, nichterfull, lizenzkriterium, stadionfra...   \n",
       "1982  [demokrat, prasidentschaftsanwarterin, fordert...   \n",
       "255   [republikan, transparenzregeln, umgang, us-pra...   \n",
       "...                                                 ...   \n",
       "5431  [premi, alexis, tsipras, neu, votum, ausgeschl...   \n",
       "2368  [kreditereignis, bmw, ausbleib, bmw, bedient, ...   \n",
       "3642  [fledermaus, viel, krankheitserreg, saugeti, b...   \n",
       "8420  [fossil, spani, ``, star, war, '', -bezug, mad...   \n",
       "6661  [general, motor, ford, setz, mehr, fahrzeug, a...   \n",
       "\n",
       "                                              text_stem  \n",
       "1637  asylverfahr soll verkurzt dafur gewerkschaft b...  \n",
       "7445  umsatz abgelauf quartal fiel 8 prozent 12,9 mr...  \n",
       "8808  weg nichterfull lizenzkriterium stadionfrag na...  \n",
       "1982  demokrat prasidentschaftsanwarterin fordert ko...  \n",
       "255   republikan transparenzregeln umgang us-praside...  \n",
       "...                                                 ...  \n",
       "5431  premi alexis tsipras neu votum ausgeschloss st...  \n",
       "2368  kreditereignis bmw ausbleib bmw bedient markt ...  \n",
       "3642  fledermaus viel krankheitserreg saugeti befall...  \n",
       "8420  fossil spani `` star war '' -bezug madrid/wi k...  \n",
       "6661  general motor ford setz mehr fahrzeug ab us-au...  \n",
       "\n",
       "[7191 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, InputLayer\n",
    "from tensorflow.keras import Sequential\n",
    "import os\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import TensorBoard\n",
    "from preprocessing.text import extract_vocabulary\n",
    "from models.text.layers import create_text_vectorization, create_word_embedding\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "    \n",
    "    \n",
    "def build_model(X_train, y_train, conv_num_filters=128, conv_kernel_size=7):\n",
    "\n",
    "    vocabulary, embedding_length = extract_vocabulary(X_train, verbose=True)\n",
    "    vectorize_layer = create_text_vectorization(vocabulary)\n",
    "    embedding_layer = create_word_embedding(vocabulary, embedding_length)\n",
    "\n",
    "    output_classes = len(label_binarizer.classes_)\n",
    "    \n",
    "    model = Sequential(name=\"cnn\")\n",
    "    model.add(InputLayer(input_shape=(1,), dtype=tf.string, name=\"text_input\"))\n",
    "    model.add(vectorize_layer)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(conv_num_filters, conv_kernel_size, activation=\"relu\", strides=1, padding=\"valid\", name=\"conv_1\"))\n",
    "    model.add(GlobalMaxPooling1D(name=\"global_max_pool_1\"))\n",
    "    model.add(Dense(output_classes, activation=tf.nn.softmax, name=\"prediction\"))\n",
    "\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.text_tokenized_stemmed\n",
    "y_train = data_train.label\n",
    "\n",
    "X_test = data_test.text_tokenized_stemmed\n",
    "y_test = data_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median sequence length:       : 173\n",
      "Percentil                     : 0.98)\n",
      "Cutoff sequence length        : 594\n",
      "Max sequence length           : 2590\n",
      "Embedding length              : 594\n",
      "Vocabulary length             : 138905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 01:42:15.731333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 01:42:15.739915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 01:42:15.740559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 01:42:15.741586: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-18 01:42:15.742393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 01:42:15.743030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 01:42:15.743608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 01:42:16.221123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 01:42:16.221877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 01:42:16.222534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-18 01:42:16.223155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13622 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "std::bad_alloc",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15877/2380113911.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbinarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabel_binarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15877/1501948762.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(X_train, y_train, conv_num_filters, conv_kernel_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mvectorize_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_text_vectorization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_word_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moutput_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_binarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/nlp-topic-classification-german/models/text/layers.py\u001b[0m in \u001b[0;36mcreate_word_embedding\u001b[0;34m(vocabulary, embedding_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_word_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_embedding_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     return Embedding(\n\u001b[1;32m     20\u001b[0m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/nlp-topic-classification-german/models/text/layers.py\u001b[0m in \u001b[0;36mcalculate_embedding_matrix\u001b[0;34m(vocabulary, embedding_dim, verbose)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedder_fasttext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/nlp-topic-classification-german/models/text/layers.py\u001b[0m in \u001b[0;36mget_embedder_fasttext\u001b[0;34m(embedding_dim, model_name)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mdownload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;34m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0meprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, args)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasttext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc"
     ]
    }
   ],
   "source": [
    "from preprocessing.categorical import binarizer\n",
    "label_binarizer = binarizer(y_train)\n",
    "model = build_model(X_train, y_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "learning_rate=0.01\n",
    "model_metric = [\"accuracy\"]\n",
    "loss_function = CategoricalCrossentropy()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=model_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "x_train = data_train['text_stem']\n",
    "y_train_bin = label_binarizer.transform(y_train)\n",
    "\n",
    "x_test = data_test['text_stem']\n",
    "y_test_bin = label_binarizer.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_input = tf.data.Dataset.from_tensor_slices((x_train, y_train_bin)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_input = tf.data.Dataset.from_tensor_slices((x_test, y_test_bin)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "train_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/fit\n",
    "\n",
    "#callbacks = [TensorBoard(\"logs/fit\", histogram_freq=1)]\n",
    "callbacks = []\n",
    "history = model.fit(train_input, validation_data=test_input, callbacks=callbacks, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reporting.training import plot_history\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = label_binarizer.inverse_transform(model.predict(x_test[0:100]))\n",
    "\n",
    "\n",
    "from reporting.evaluation import plot_confusion_matrix\n",
    "plot_confusion_matrix(y_test[0:100], y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
