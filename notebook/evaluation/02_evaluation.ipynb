{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c3c405-e4d9-473b-a314-771baaabe8d0",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb8597f-d94c-4ef7-beea-659316adb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pandas import concat, set_option, DataFrame\n",
    "\n",
    "from data import file\n",
    "from reporting.evaluation import plot_confusion_matrix, percentage_true_positives"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Prediction Data\n",
    "Within each model I stored 100 samples to generate confusion matrices and statistics here.\n",
    "It allowed me to generate metrics and comparisons here. The stored data has the following format and is under\n",
    "[version control](../../data/processed/)\n",
    "\n",
    "```\n",
    "{\n",
    "\"expected\": [\"International\", \"Etat\", ...],\n",
    "\"predicted\": [\"International\", \"Etat\", ...]\n",
    "}```\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e56304fa-c2ae-4e85-912c-6ff057ffc1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file.reporting_data_report_tfidf, 'r') as f:\n",
    "    tfidf_raw = json.load(f)\n",
    "    tfidf_expected = tfidf_raw['expected']\n",
    "    tfidf_predicted = tfidf_raw['predicted']\n",
    "\n",
    "with open(file.reporting_data_report_cnn, 'r') as f:\n",
    "    cnn_raw = json.load(f)\n",
    "    cnn_expected = cnn_raw['expected']\n",
    "    cnn_predicted = cnn_raw['predicted']\n",
    "\n",
    "with open(file.reporting_data_report_rnn, 'r') as f:\n",
    "    rnn_raw = json.load(f)\n",
    "    rnn_expected = rnn_raw['expected']\n",
    "    rnn_predicted = rnn_raw['predicted']\n",
    "\n",
    "with open(file.reporting_data_report_bert, 'r') as f:\n",
    "    bert_raw = json.load(f)\n",
    "    bert_expected = bert_raw['expected']\n",
    "    bert_predicted = bert_raw['predicted']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Result\n",
    "During model development and training I mainly focused on the recall (sensitivity / true positive rate). The obvious\n",
    "reason is that it's the metric I am most used to and I was not aware that deciding which metric to use might be an active\n",
    "decision to do upfront. On the other hand it feels quite obvious to use recall considering the type of Problem to solve.\n",
    "\n",
    "The following table shows a comparision of recalls for models that I investigated:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                   TF-IDF        CNN        RNN        BERT\nEtat            66.667000  83.333000  83.333000   66.667000\nInland          75.000000  37.500000  50.000000  100.000000\nInternational  100.000000  50.000000  70.588000  100.000000\nKultur         100.000000  40.000000  20.000000  100.000000\nPanorama        79.167000  75.000000  57.143000   79.167000\nSport           90.909000  72.727000  87.500000   90.909000\nWeb             92.308000  69.231000  68.182000   84.615000\nWirtschaft      84.615000  46.154000  61.538000   84.615000\nWissenschaft   100.000000  60.000000   0.000000   80.000000\nMean            87.629556  59.327222  55.364889   87.330333",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TF-IDF</th>\n      <th>CNN</th>\n      <th>RNN</th>\n      <th>BERT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Etat</th>\n      <td>66.667000</td>\n      <td>83.333000</td>\n      <td>83.333000</td>\n      <td>66.667000</td>\n    </tr>\n    <tr>\n      <th>Inland</th>\n      <td>75.000000</td>\n      <td>37.500000</td>\n      <td>50.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>International</th>\n      <td>100.000000</td>\n      <td>50.000000</td>\n      <td>70.588000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>Kultur</th>\n      <td>100.000000</td>\n      <td>40.000000</td>\n      <td>20.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>Panorama</th>\n      <td>79.167000</td>\n      <td>75.000000</td>\n      <td>57.143000</td>\n      <td>79.167000</td>\n    </tr>\n    <tr>\n      <th>Sport</th>\n      <td>90.909000</td>\n      <td>72.727000</td>\n      <td>87.500000</td>\n      <td>90.909000</td>\n    </tr>\n    <tr>\n      <th>Web</th>\n      <td>92.308000</td>\n      <td>69.231000</td>\n      <td>68.182000</td>\n      <td>84.615000</td>\n    </tr>\n    <tr>\n      <th>Wirtschaft</th>\n      <td>84.615000</td>\n      <td>46.154000</td>\n      <td>61.538000</td>\n      <td>84.615000</td>\n    </tr>\n    <tr>\n      <th>Wissenschaft</th>\n      <td>100.000000</td>\n      <td>60.000000</td>\n      <td>0.000000</td>\n      <td>80.000000</td>\n    </tr>\n    <tr>\n      <th>Mean</th>\n      <td>87.629556</td>\n      <td>59.327222</td>\n      <td>55.364889</td>\n      <td>87.330333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_tfidf = percentage_true_positives(tfidf_predicted, tfidf_expected, column='TF-IDF')\n",
    "tp_cnn = percentage_true_positives(cnn_predicted, cnn_expected, column='CNN')\n",
    "tp_rnn = percentage_true_positives(rnn_predicted, rnn_expected, column='RNN')\n",
    "tp_bert = percentage_true_positives(bert_predicted, bert_expected, column='BERT')\n",
    "set_option('display.max_rows', 500)\n",
    "set_option('display.max_columns', 500)\n",
    "set_option('display.width', 1000)\n",
    "set_option('display.expand_frame_repr', True)\n",
    "\n",
    "table = concat([tp_tfidf, tp_cnn, tp_rnn, tp_bert], axis=1)\n",
    "table.loc['Mean'] = table.mean()\n",
    "table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Base Model\n",
    "For the base model predictions I was using TF-IDF with preprocessed data. During preprocessing I generated raw, lemmatized\n",
    "and stemmed tokens. Out of this I got best average recall for the lemmatized input and decided to use that.\n",
    "With an average recall of around 87% TF-IDF was in my opinion already providing a very good base model performance.\n",
    "\n",
    "#### Libraries: sklearn, matplotlib, pandas, seaborn\n",
    "\n",
    "### Neural Networks\n",
    "While trying neural networks the main questions to answer were the following:\n",
    "- Is the out-of-the-box performance of a given neural network much better than the one from the base model?\n",
    "- How good is the trainability of a given network? Or in other words, how much can I improve the performance by tuning?\n",
    "\n",
    "### CNN\n",
    "I was building a CNN using a pretrained fasttext word embeddings followed by convolutional and max pooling layer.\n",
    "The out-of-the-box average recall was at around 59% and much worse than the base model.\n",
    "\n",
    "#### Layers\n",
    "1. Input Layer\n",
    "2. Text Vectorization\n",
    "3. Word Embedding (fasttext, pretrained)\n",
    "4. Convolutional\n",
    "5. Global Max Pooling\n",
    "6. Dense (softmax)\n",
    "\n",
    "#### Libraries: tensorflow, sklearn, matplotlib, pandas, seaborn\n",
    "\n",
    "#### Decision:\n",
    "I expected to get better performance using RNNs and LSTMs out of the fact that this were more traditional approaches\n",
    "for text classification prior 2018. That was the main reason to not spend lots of time in tuning.\n",
    "\n",
    "### RNN\n",
    "The RNN I built was based on a word embedding layer (fasttext) followed by a bidirectional LSTM layer containing 32 units.\n",
    "The out-of-the-box average recall was at around 55%.\n",
    "\n",
    "#### Layers\n",
    "1. Input Layer\n",
    "2. Text Vectorization\n",
    "3. Word Embedding (fasttext, pretrained)\n",
    "4. Bidirectional LSTM\n",
    "5. Global Max Pooling\n",
    "6. Dense (softmax)\n",
    "\n",
    "#### Libraries: tensorflow, sklearn, matplotlib, pandas, seaborn\n",
    "\n",
    "#### Tuning\n",
    "I did tuning based on tensorflow board hparams which was one of the native tensorflow approaches I found. The tuning was\n",
    "mainly based on the following parameters:\n",
    "- number of LSTM layers\n",
    "- nuber of LSTM units\n",
    "- bidirectional / unidirectional\n",
    "\n",
    "#### Decision\n",
    "The tuning did not end up very successfull and the model performance (recall) trend was downwards. Because of that I\n",
    "decided to stop tuning and give transformers a try.\n",
    "\n",
    "### BERT Transformer\n",
    "In order to test transformer approaches I was using a pretrained tokenizer and model (bert-base-german-cased) from\n",
    "[huggingface](https://huggingface.co/bert-base-german-cased). The model was available for PyTorch and Tensorflow and I\n",
    "decided to use Tensorflow. For activation I used softmax.\n",
    "The out-of-the-box average recall was at around 87%\n",
    "\n",
    "\n",
    "#### Layers\n",
    "0. Pretrained Bert Tokenizer (bert-base-german-cased)\n",
    "1. Input Layer\n",
    "2. Pretrained Bert Model (bert-base-german-cased)\n",
    "3. Dense (softmax)\n",
    "\n",
    "#### Libraries: tensorflow, transformers, sklearn, matplotlib, pandas, seaborn\n",
    "\n",
    "\n",
    "## Recommendation\n",
    "TF-IDF and Transformers have similar performance. Because of that I would recommend one of the following options:\n",
    "\n",
    "A) If the performance of around 87% is acceptable I would go with TF-IDF because of the following reasons:\n",
    "   - simpler, less resource intensive, faster, less expensive\n",
    "   - TF-IDF is better explainable than a neural network\n",
    "\n",
    "B) If performance of around 87% is not good enough I would go and tune transformers due to the following reasons:\n",
    "   - I would expect the models to have more room for improvement than my current base model based on TF-IDF\n",
    "   - Last but not least: for a startup I would definitly go with ML because it increases the value of the company ;)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m84"
  },
  "kernelspec": {
   "name": "pycharm-d7199a65",
   "language": "python",
   "display_name": "PyCharm (nlp-topic-classification-german)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}